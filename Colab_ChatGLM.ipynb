{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D9iIjZvZeEEo",
        "E3RnfVUceRM5",
        "akGK5YpRjcu_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f036a994baf43db8a07ca677d4ad328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "ChatGLM:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1fe2fd01a87a4564b57e084c442c407f",
            "placeholder": "å›ç­”...",
            "rows": 2,
            "style": "IPY_MODEL_24c561f60e6547b6b46cc8ed0118913b",
            "value": "æ·˜å®å®¢æœè®©ä½ è½¬è´¦ï¼Œå¯èƒ½å­˜åœ¨è¯ˆéª—é£é™©ã€‚å› ä¸ºæ·˜å®å®¢æœä¸ä¼šè‡ªå·±å‘ä½ çš„é“¶è¡Œè´¦æˆ·è½¬è´¦ï¼Œé™¤éä»–ä»¬å·²ç»æœ‰äº†ä½ çš„é“¶è¡Œè´¦æˆ·ä¿¡æ¯ã€‚å¦‚æœæ‹…å¿ƒå®‰å…¨é£é™©ï¼Œå»ºè®®å…ˆç¡®è®¤å¯¹æ–¹çš„èº«ä»½ï¼Œå¹¶æ ¸å®å…¶æä¾›çš„é“¶è¡Œè´¦æˆ·ä¿¡æ¯ã€‚åœ¨ç¡®è®¤ä¿¡æ¯æ— è¯¯çš„æƒ…å†µä¸‹ï¼Œæ‰è¿›è¡Œè½¬è´¦æ“ä½œã€‚å¯ä»¥è®°å½•ä¸‹è¿™ä¸ªå®¢æœçš„ç”µè¯å·ç å’Œå§“åï¼Œä¸¾æŠ¥ç»™æ·˜å®å®¢æœå¹³å°ã€‚"
          }
        },
        "1fe2fd01a87a4564b57e084c442c407f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "0 1 auto",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "auto",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": "40px",
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "1400px"
          }
        },
        "24c561f60e6547b6b46cc8ed0118913b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8298dee0c2e94655bb22fbca2583a3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "ç»ˆæ­¢å›ç­”",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_48dc4a84269f41e69a4a5e9767d685c4",
            "style": "IPY_MODEL_2b60a6eae56f4baa8ee1d5efd73a9d9b",
            "tooltip": ""
          }
        },
        "48dc4a84269f41e69a4a5e9767d685c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b60a6eae56f4baa8ee1d5efd73a9d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d09cab66c8af4bfaa3857d6fa836555f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "ChatGLM:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4c3db9fc83b0443e98fe8206ac4f9dd1",
            "placeholder": "å›ç­”...",
            "rows": 1,
            "style": "IPY_MODEL_06fffb59b0054caf99e6c9dca75921f7",
            "value": "æˆ‘ä½œä¸ºä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæ²¡æœ‰å®é™…çš„æƒ…æ„Ÿå’Œæ„è¯†ï¼Œåªèƒ½å›ç­”æ‚¨çš„é—®é¢˜å’Œæä¾›å¸®åŠ©ã€‚ä½†æ˜¯ï¼Œæˆ‘å¯ä»¥å‘Šè¯‰æ‚¨ï¼Œè¡¨è¾¾çˆ±æ˜¯ä¸€ç§ç¾å¥½å’Œæ¸©æš–çš„æƒ…æ„Ÿï¼Œå¸Œæœ›æ‚¨èƒ½æ„Ÿå—åˆ°å®ƒã€‚"
          }
        },
        "4c3db9fc83b0443e98fe8206ac4f9dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "0 1 auto",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "auto",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": "100px",
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "06fffb59b0054caf99e6c9dca75921f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanzhang9999/Awesome-ChatGPT-AIGC-Lesson/blob/main/Colab_ChatGLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=wsh.colab_chatglm)\n",
        "\n",
        "># **åŸºäºæ¸…åå¤§å­¦å‘å¸ƒçš„å¯¹è¯è¯­è¨€æ¨¡å‹ChatGLM**\n",
        "> Colab notebook by [Happy_WSH](https://space.bilibili.com/8417436)ã€‚\n",
        "\n",
        ">æˆ‘åšäº†ä»€ä¹ˆï¼Ÿ\n",
        ">\n",
        ">ç¼–å†™æ­¤notebookåŠæ“ä½œæ•™ç¨‹ï¼›ç¼–å†™äº†Colabæ“ä½œé£æ ¼çš„WSHæ–¹æ³•ï¼›æµ‹è¯•å¹¶ç»™å‡ºé€‚åˆColabå…è´¹ç”¨æˆ·ä½¿ç”¨çš„é…ç½®\n",
        "\n",
        "æ¨¡å‹ä½œè€…githubï¼š[THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)\n",
        "\n",
        "ç§‹å¶æ–¹æ³•githubï¼š[Akegarasu/ChatGLM-webui](https://github.com/Akegarasu/ChatGLM-webui)\n",
        "\n",
        "WSHæ–¹æ³•githubï¼š[WSH032/ChatGLM-6](https://github.com/WSH032/ChatGLM-6B)\n",
        "\n",
        "ä½¿ç”¨æ—¶è¯·éµå®ˆApache-2.0 licenseåè®®ï¼ŒåŠå„githubä»“åº“è¦æ±‚çš„åè®®\n",
        "\n",
        "`ä¸è¿‡ï¼Œç”±äº ChatGLM-6B çš„è§„æ¨¡è¾ƒå°ï¼Œç›®å‰å·²çŸ¥å…¶å…·æœ‰ç›¸å½“å¤šçš„å±€é™æ€§ï¼Œå¦‚äº‹å®æ€§/æ•°å­¦é€»è¾‘é”™è¯¯ï¼Œå¯èƒ½ç”Ÿæˆæœ‰å®³/æœ‰åè§å†…å®¹ï¼Œè¾ƒå¼±çš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œè‡ªæˆ‘è®¤çŸ¥æ··ä¹±ï¼Œä»¥åŠå¯¹è‹±æ–‡æŒ‡ç¤ºç”Ÿæˆä¸ä¸­æ–‡æŒ‡ç¤ºå®Œå…¨çŸ›ç›¾çš„å†…å®¹ã€‚è¯·å¤§å®¶åœ¨ä½¿ç”¨å‰äº†è§£è¿™äº›é—®é¢˜ï¼Œä»¥å…äº§ç”Ÿè¯¯è§£ã€‚æ›´å¤§çš„åŸºäº 1300 äº¿å‚æ•° GLM-130B çš„ ChatGLM æ­£åœ¨å†…æµ‹å¼€å‘ä¸­ã€‚---æ¨¡å‹ä½œè€…THUDM`\n"
      ],
      "metadata": {
        "id": "mYraIiRZzU40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (ä¸€)å…‹éš†githubï¼Œå®‰è£…ä¾èµ–ï¼Œé…ç½®ç¯å¢ƒ"
      ],
      "metadata": {
        "id": "BF-6N5YbIxAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##*æŒ‚è½½è°·æ­Œç¡¬ç›˜ï¼ˆå¯é€‰ï¼‰*\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UjcLVSLBLdHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21be1b90-0dc4-4b7b-c99b-8f7afa90892d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "52QXFL73_0Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7dc26d-4440-4cc7-c695-4a51d717d54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatGLM-webui'...\n",
            "remote: Enumerating objects: 353, done.\u001b[K\n",
            "remote: Counting objects: 100% (167/167), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 353 (delta 103), reused 116 (delta 82), pack-reused 186\u001b[K\n",
            "Receiving objects: 100% (353/353), 87.82 KiB | 2.14 MiB/s, done.\n",
            "Resolving deltas: 100% (208/208), done.\n",
            "/content/ChatGLM-webui\n",
            "æ­£åœ¨å®‰è£…ä¾èµ–ï¼Œè¯·è€å¿ƒç­‰å¾…\n",
            "ä¾èµ–å®‰è£…å®Œæˆ\n",
            "UTF-8\n",
            "UTF-8\n"
          ]
        }
      ],
      "source": [
        "#@title ##1.1å…‹éš†ç§‹å¶çš„åº“ã€å®‰è£…ä¾èµ–\n",
        "\n",
        "!git clone https://github.com/Akegarasu/ChatGLM-webui\n",
        "%cd /content/ChatGLM-webui\n",
        "print(f\"æ­£åœ¨å®‰è£…ä¾èµ–ï¼Œè¯·è€å¿ƒç­‰å¾…\")\n",
        "!pip install --upgrade -r requirements.txt  > /dev/null 2>&1\n",
        "print(f\"ä¾èµ–å®‰è£…å®Œæˆ\")\n",
        "\n",
        "#åˆ‡æ¢ç¼–ç \n",
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "print(locale.getpreferredencoding())\n",
        "import os\n",
        "os.environ['PYTHONIOENCODING'] = 'UTF-8'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *å¦‚æœä½ ä½¿ç”¨CPUæ¨¡å‹ï¼Œè¯·æŒ‰ç…§å¦‚ä¸‹æ•™ç¨‹(å¯é€‰)*"
      ],
      "metadata": {
        "id": "C_dgVputCfzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#è¿è¡Œè¿™ä¸ªä»£ç æ‰¾åˆ°libcuda.so.1æ–‡ä»¶\n",
        "!sudo find /usr/ -name 'libcuda.so.1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1ySfC1XCm0l",
        "outputId": "678af08e-0fed-4043-9a18-88c1ea9d4a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.8/compat/libcuda.so.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#è¿è¡Œè¿™ä¸ªä»£ç æ‰¾åˆ°ç¯å¢ƒè·¯å¾„(è¢«\":\"åˆ†å‰²ä¸ºä¸¤ä¸ª)\n",
        "!echo $LD_LIBRARY_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13mv1tejDSAw",
        "outputId": "da7f634f-35aa-4a2c-c012-18a3666f5d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title å°†cudaæ–‡ä»¶ï¼Œä¸¤ä¸ªç¯å¢ƒè·¯å¾„å¡«å…¥ä»£ç å—ï¼Œå¹¶è¿è¡Œ\n",
        "cuda_file_path = \"/usr/local/cuda-11.8/compat/libcuda.so.1\" #@param {type:\"string\"}\n",
        "environment_path1 = \"/usr/local/nvidia/lib\" #@param {type:\"string\"}\n",
        "environment_path2 = \"/usr/local/nvidia/lib64\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "!mkdir -p {environment_path1}\n",
        "!mkdir -p {environment_path2}\n",
        "\n",
        "!cp -p {cuda_file_path} {environment_path1}\n",
        "!cp -p {cuda_file_path} {environment_path2}\n",
        "\n",
        "print(\"cudaæ–‡ä»¶æ‹·è´å®Œæˆ\")\n",
        "\n",
        "#@markdown **!!!è¿è¡Œå®Œåï¼Œç‚¹å‡»`ä»£ç æ‰§è¡Œç¨‹åº`--`é‡æ–°å¯åŠ¨ä»£ç æ‰§è¡Œç¨‹åº`å®Œæˆåˆ·æ–°!!!**"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WBmCewXMBVTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ï¼ˆäºŒï¼‰é€‰æ‹©ä¸€ä¸ªæ–¹æ³•å¹¶ä½¿ç”¨"
      ],
      "metadata": {
        "id": "noglVVk7d7xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1ç§‹å¶æ–¹æ³•ï¼ˆWebUIé£æ ¼ï¼‰`åŠŸèƒ½å¤šï¼Œä½†gradioåœ¨Colabé‡Œä¸ç¨³å®š`"
      ],
      "metadata": {
        "id": "D9iIjZvZeEEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ä¿®å¤bug https://github.com/googlecolab/colabtools/issues/3409\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "k_DjaJVRvhmH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.1.1 è®¾ç½®å‚æ•°å¹¶å¼€å§‹å¯¹è¯\n",
        "\n",
        "%cd /content/ChatGLM-webui\n",
        "\n",
        "extArgs=\"\"\n",
        "\n",
        "#@markdown æ˜¯å¦ç”¨CPUè¿›è¡Œæ¨ç†`CPUæ¨¡å¼è¯·é€‰æ‹©chatglm-6b-int4-qeï¼Œä¸ç„¶ä¼šçˆ†ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "#å¯ç”¨CPUæ¨¡å‹\n",
        "if use_cpu:\n",
        "  extArgs = extArgs + \"--cpu \"\n",
        "\n",
        "#@markdown é€‰æ‹©æ¨¡å‹`Colabå…è´¹ç”¨æˆ·åªèƒ½ä½¿ç”¨int4å’Œqeæ¨¡å‹`ï¼Œæˆ–è€…å¡«å…¥è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„`å°†ä¼šè¦†ç›–é¢„è®¾æ¨¡å‹é€‰æ‹©`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#ç”¨è‡ªå®šä¹‰è·¯å¾„è¦†ç›–é¢„è®¾\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "#@markdown æ¨ç†ç²¾åº¦`ç•™ç©ºåˆ™è‡ªåŠ¨æŒ‡å®š, fp32åªæœ‰CPUå¯ä½¿ç”¨ ï¼› int4ã€int8åªæœ‰GPUèƒ½ç”¨`\n",
        "precision = \"int8\" #@param [\"\", \"fp32\", \"fp16\", \"int4\", \"int8\"]\n",
        "#æŒ‡å®šç²¾åº¦\n",
        "if precision:\n",
        "  extArgs = extArgs + f\"--precision={precision} \"\n",
        "\n",
        "#å¯åŠ¨\n",
        "!python webui.py --model-path={model_path} --listen --share {extArgs}"
      ],
      "metadata": {
        "id": "jIiNeWyKAL6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6b9dca-4482-4a21-b076-78a14801572f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ChatGLM-webui\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.so\n",
            "Kernels compiled : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2WSHæ–¹æ³•ï¼ˆColabé£æ ¼ï¼‰`åœ¨Colabé‡Œå…¼å®¹è¾ƒå¥½`"
      ],
      "metadata": {
        "id": "E3RnfVUceRM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.1é€‰æ‹©å¹¶å¯ç”¨æ¨¡å‹\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "#@markdown æ˜¯å¦ç”¨CPUè¿›è¡Œæ¨ç†`CPUæ¨¡å¼è¯·é€‰æ‹©chatglm-6b-int4-qeï¼Œä¸ç„¶ä¼šçˆ†ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown é€‰æ‹©æ¨¡å‹`Colabå…è´¹ç”¨æˆ·åªèƒ½ä½¿ç”¨int4å’Œqeæ¨¡å‹`ï¼Œæˆ–è€…å¡«å…¥è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„`å°†ä¼šè¦†ç›–é¢„è®¾æ¨¡å‹é€‰æ‹©`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#ç”¨è‡ªå®šä¹‰è·¯å¾„è¦†ç›–é¢„è®¾\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "if use_cpu:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).float()\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "#åˆå§‹åŒ–å‚æ•°\n",
        "history = []\n",
        "count = 1\n",
        "max_length = 2048\n",
        "top_p = 0.7\n",
        "temperature =  0.95\n",
        "max_turns = 20\n",
        "clear_history_flag = False\n",
        "\n",
        "def clear_history_set():\n",
        "  global history, count, clear_history_flag\n",
        "  history = []\n",
        "  count = 1\n",
        "  clear_history_flag = False"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m_FzCEwuUvKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9d450a-ff9e-4adb-8d69-114d525fcf35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.so\n",
            "Kernels compiled : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/9333486c30bfc3860052b7ff4b2c006b576dcb4c/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.2å‚æ•°è®¾ç½®ï¼ˆå¯¹è¯ä¸­ä¹Ÿå¯æ›´æ”¹ï¼‰\n",
        "max_length = 2048 #@param {type:\"number\"}\n",
        "top_p = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "temperature =  0.95 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "max_turns = 20 #@param {type:\"slider\", min:1, max:256, step:1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pqaSs3MgbnV4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.3æé—®\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from ipywidgets import Layout\n",
        "\n",
        "#@markdown æé—®\n",
        "ask = \"\\u4F60\\u597D,\\u6DD8\\u5B9D\\u5BA2\\u670D\\u7ED9\\u6211\\u6253\\u7535\\u8BDD\\uFF0C\\u8BA9\\u6211\\u8F6C\\u8D26\\uFF0C\\u8FD9\\u4E2A\\u6709\\u8BC8\\u9A97\\u98CE\\u9669\\u5417\" #@param {type:\"string\"}\n",
        "\n",
        "#æ¸…ç©ºå†å²\n",
        "if clear_history_flag:\n",
        "  clear_history_set()\n",
        "  print(f\"è¾¾åˆ°å¯¹è¯æ¬¡æ•°ä¸Šé™ï¼Œå†å²å¯¹è¯è®°å½•å·²è¢«æ¸…ç©º\")\n",
        "\n",
        "print(f\"ç¬¬ {count} æ¬¡å¯¹è¯ï¼Œåˆ°è¾¾ {max_turns} æ¬¡åï¼Œä¸‹ä¸€æ¬¡å¯¹è¯æ—¶å°†åˆ é™¤å†å²å¯¹è¯è®°å½•\")\n",
        "\n",
        "#ç”¨äºæ ¹æ®æ–‡æœ¬æ¡†è¡Œæ•°è‡ªåŠ¨è°ƒæ•´é«˜åº¦\n",
        "def get_bigger (args):\n",
        "  a = textarea_param.value.count ('\\n') + 1\n",
        "  b = int(len(textarea_param.value) / 100 ) +1\n",
        "  l = max(a,b)\n",
        "  textarea_param.rows = l\n",
        "\n",
        "#è®¾ç½®æ–‡æœ¬æ¡†å¤§å°\n",
        "text_layout = Layout (flex='0 1 auto', height='auto', min_height='40px', width='1400px')\n",
        "textarea_param = widgets.Textarea (value='', placeholder='å›ç­”...', description='ChatGLM:', disabled=False, layout=text_layout)\n",
        "textarea_param.observe (get_bigger, 'value')\n",
        "display(textarea_param)\n",
        "\n",
        "#é€šè¿‡ipywidgetsè¾“å‡ºå›ç­”å†…å®¹\n",
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    textarea_param.value = response\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "  return old_response, old_history\n",
        "\n",
        "\n",
        "#æé—®åŠè®¡æ•°+1\n",
        "response, history = ask_and_ans(tokenizer, ask, history, max_length=max_length, top_p=top_p,temperature=temperature)\n",
        "count = count +1\n",
        "\n",
        "#è®¾ç½®æ¸…ç©ºæ ‡å¿—\n",
        "if count > max_turns:\n",
        "  clear_history_flag = True\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XYEVkeAqduqL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253,
          "referenced_widgets": [
            "2f036a994baf43db8a07ca677d4ad328",
            "1fe2fd01a87a4564b57e084c442c407f",
            "24c561f60e6547b6b46cc8ed0118913b"
          ]
        },
        "outputId": "3944ac1f-1829-4563-8191-a8579b6fecea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç¬¬ 2 æ¬¡å¯¹è¯ï¼Œåˆ°è¾¾ 20 æ¬¡åï¼Œä¸‹ä¸€æ¬¡å¯¹è¯æ—¶å°†åˆ é™¤å†å²å¯¹è¯è®°å½•\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='ChatGLM:', layout=Layout(flex='0 1 auto', height='auto', min_height='40px', wiâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f036a994baf43db8a07ca677d4ad328"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.2.4 å†å²å¯¹è¯è®°å½•\n",
        "\n",
        "\n",
        "\n",
        "#@markdown æ˜¾ç¤ºå¯¹è¯å†å²\n",
        "show_history = True #@param {type:\"boolean\"}\n",
        "if show_history:\n",
        "  for ask_contet,ans_content in history:\n",
        "    print(f\"ç”¨æˆ·ï¼š {ask_contet}\")\n",
        "    print(f\"å›ç­”ï¼š {ans_content}\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "#@markdown æ‰‹åŠ¨æ¸…ç©ºå¯¹è¯å†å²\n",
        "clear_history = False #@param {type:\"boolean\"}\n",
        "if clear_history:\n",
        "  clear_history_set()\n",
        "  print(f\"èŠå¤©è®°å½•å·²è¢«æ¸…ç©º\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kTAUdZZ1kniH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233a5dac-37b1-4342-f022-408490b5246e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç”¨æˆ·ï¼š ä½ å¥½\n",
            "å›ç­”ï¼š ä½ å¥½ï¼ æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å›ç­”ä½ çš„é—®é¢˜ã€‚è¯·é—®æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼Ÿ\n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3å®˜æ–¹æµå¼æ–¹æ³•"
      ],
      "metadata": {
        "id": "akGK5YpRjcu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.3.1é€‰æ‹©å¹¶å¯ç”¨æ¨¡å‹\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "#@markdown æ˜¯å¦ç”¨CPUè¿›è¡Œæ¨ç†`CPUæ¨¡å¼è¯·é€‰æ‹©chatglm-6b-int4-qeï¼Œä¸ç„¶ä¼šçˆ†ram`\n",
        "use_cpu = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown é€‰æ‹©æ¨¡å‹`Colabå…è´¹ç”¨æˆ·åªèƒ½ä½¿ç”¨int4å’Œqeæ¨¡å‹`ï¼Œæˆ–è€…å¡«å…¥è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„`å°†ä¼šè¦†ç›–é¢„è®¾æ¨¡å‹é€‰æ‹©`\n",
        "model_path = \"THUDM/chatglm-6b-int4\" #@param [\"THUDM/chatglm-6b\", \"THUDM/chatglm-6b-int4\", \"THUDM/chatglm-6b-int4-qe\"]\n",
        "your_model_path = \"\" #@param {type:\"string\"}\n",
        "#ç”¨è‡ªå®šä¹‰è·¯å¾„è¦†ç›–é¢„è®¾\n",
        "if your_model_path:\n",
        "  model_path = your_model_path\n",
        "\n",
        "if use_cpu:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).float()\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "  model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
        "\n",
        "model = model.eval()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mRlNHn6FmATu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###2.3.2å¼€å¯å¯¹è¯\n",
        "\n",
        "#@markdown æœ€å¤§å¯¹è¯è½®æ•°\n",
        "MAX_TURNS = 20 #@param {type:\"slider\", min:1, max:256, step:1}\n",
        "MAX_BOXES = MAX_TURNS * 2\n",
        "\n",
        "\n",
        "\n",
        "def predict(input, max_length, top_p, temperature, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,\n",
        "                                               temperature=temperature):\n",
        "        updates = []\n",
        "        for query, response in history:\n",
        "            updates.append(gr.update(visible=True, value=\"ç”¨æˆ·ï¼š\" + query))\n",
        "            updates.append(gr.update(visible=True, value=\"ChatGLM-6Bï¼š\" + response))\n",
        "        if len(updates) < MAX_BOXES:\n",
        "            updates = updates + [gr.Textbox.update(visible=False)] * (MAX_BOXES - len(updates))\n",
        "        yield [history] + updates\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    state = gr.State([])\n",
        "    text_boxes = []\n",
        "    for i in range(MAX_BOXES):\n",
        "        if i % 2 == 0:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"æé—®ï¼š\"))\n",
        "        else:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"å›å¤ï¼š\"))\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\", lines=11).style(\n",
        "                container=False)\n",
        "        with gr.Column(scale=1):\n",
        "            max_length = gr.Slider(0, 4096, value=2048, step=1.0, label=\"Maximum length\", interactive=True)\n",
        "            top_p = gr.Slider(0, 1, value=0.7, step=0.01, label=\"Top P\", interactive=True)\n",
        "            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n",
        "            button = gr.Button(\"Generate\")\n",
        "    button.click(predict, [txt, max_length, top_p, temperature, state], [state] + text_boxes)\n",
        "demo.queue().launch()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nBSLEcXyjoBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *ï¼ˆä¸‰ï¼‰å¼€å‘å¤‡ç”¨ä»£ç *"
      ],
      "metadata": {
        "id": "HSWWiFDvjBn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "old_response = \"\"\n",
        "for response, history in model.stream_chat(tokenizer, \"ä½ å¥½\", [], max_length=2048, top_p=0.7, temperature=0.95):\n",
        "  print(response[len(old_response):], end=\"\")\n",
        "  old_response = response\n",
        "print(end=\"\\r\")\n",
        "print(old_response)"
      ],
      "metadata": {
        "id": "5jh610GcQe5m",
        "outputId": "14bb8813-cea7-46af-be74-313a5023ac17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "old_response = \"\"\n",
        "for response, history in model.stream_chat(tokenizer, \"ä½ å¥½\", [], max_length=2048, top_p=0.7, temperature=0.95):\n",
        "  old_response = response\n",
        "  sys.stdout.write(response)\n",
        "  sys.stdout.flush()\n",
        "  sys.stdout.write(\"\\r\")\n",
        "print(old_response) #"
      ],
      "metadata": {
        "id": "Rtju7dXTRc9E",
        "outputId": "bbdee23d-00b5-481d-dd28-15875f8bdd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "    print(end=\"\\r\")\n",
        "    print(response, end=\"\", flush=True) # æ‰“å°å½“å‰å­—ç¬¦ä¸²\n",
        "  print(end=\"\\r\")\n",
        "  print(old_response)\n",
        "  return old_response, old_history"
      ],
      "metadata": {
        "id": "-r0JZsMKB-aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    print(response[len(old_response):], end=\"\")\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "  output.clear()\n",
        "  print(old_response)\n",
        "  return old_response, old_history"
      ],
      "metadata": {
        "id": "2EE60jIPB-bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###ç»ˆæ­¢æŒ‰é’®\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from ipywidgets import Layout\n",
        "import threading\n",
        "\n",
        "ask = \"\\u6211\\u7231\\u4F60\" #@param {type:\"string\"}\n",
        "\n",
        "#ç”¨äºç»ˆæ­¢ask_and_ans\n",
        "stop_flag = False\n",
        "\n",
        "#æ¸…ç©ºå†å²\n",
        "if clear_history_flag:\n",
        "  clear_history_set()\n",
        "  print(f\"è¾¾åˆ°å¯¹è¯æ¬¡æ•°ä¸Šé™ï¼Œå†å²å¯¹è¯è®°å½•å·²è¢«æ¸…ç©º\")\n",
        "\n",
        "print(f\"ç¬¬ {count} æ¬¡å¯¹è¯ï¼Œåˆ°è¾¾ {max_turns} æ¬¡åï¼Œä¸‹ä¸€æ¬¡å¯¹è¯æ—¶å°†åˆ é™¤å†å²å¯¹è¯è®°å½•\")\n",
        "\n",
        "\n",
        "# å®šä¹‰ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œå®ƒä¼šåœ¨æŒ‰é’®è¢«ç‚¹å‡»æ—¶è°ƒç”¨\n",
        "def stop_button_clicked(b):\n",
        "    # ä½¿ç”¨globalå…³é”®å­—ï¼Œå£°æ˜è¦ä¿®æ”¹å…¨å±€å˜é‡stop_flagçš„å€¼\n",
        "    global stop_flag\n",
        "    # å°†stop_flagçš„å€¼è®¾ä¸ºTrueï¼Œè¡¨ç¤ºè¦ç»ˆæ­¢å­çº¿ç¨‹\n",
        "    print(\"ç»ˆæ­¢å›ç­”.\")\n",
        "    stop_flag = True\n",
        "\n",
        "# åˆ›å»ºä¸€ä¸ªæŒ‰é’®æ§ä»¶ï¼Œå¹¶ä¸ºå®ƒæ³¨å†Œå›è°ƒå‡½æ•°\n",
        "stop_button = widgets.Button(description=\"ç»ˆæ­¢å›ç­”\")\n",
        "stop_button.on_click(stop_button_clicked)\n",
        "# æ˜¾ç¤ºæŒ‰é’®æ§ä»¶\n",
        "display(stop_button)\n",
        "\n",
        "\n",
        "#ç”¨äºæ ¹æ®æ–‡æœ¬æ¡†è¡Œæ•°è‡ªåŠ¨è°ƒæ•´é«˜åº¦\n",
        "def get_bigger (args):\n",
        "  textarea_param.rows = textarea_param.value.count ('\\n') + 1\n",
        "\n",
        "#è®¾ç½®æ–‡æœ¬æ¡†å¤§å°\n",
        "text_layout = Layout (flex='0 1 auto', height='auto', min_height='100px', width='auto')\n",
        "textarea_param = widgets.Textarea (value='', placeholder='å›ç­”...', description='ChatGLM:', disabled=False, layout=text_layout)\n",
        "textarea_param.observe (get_bigger, 'value')\n",
        "display(textarea_param)\n",
        "\n",
        "#é€šè¿‡ipywidgetsè¾“å‡ºå›ç­”å†…å®¹\n",
        "def ask_and_ans(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "  global stop_flag\n",
        "  old_history = \"\"\n",
        "  old_response = \"\"\n",
        "  for response, history in model.stream_chat(tokenizer, ask, history, max_length, top_p, temperature):\n",
        "    textarea_param.value = response\n",
        "    old_response = response\n",
        "    old_history = history\n",
        "    if stop_flag:\n",
        "      # è·å–å½“å‰çº¿ç¨‹å®ä¾‹\n",
        "      thread = threading.current_thread()\n",
        "      # å°†è¿”å›å€¼èµ‹ç»™resultå±æ€§\n",
        "      thread.result = (old_response, old_history)\n",
        "      return old_response, old_history\n",
        "  # è·å–å½“å‰çº¿ç¨‹å®ä¾‹\n",
        "  thread = threading.current_thread()\n",
        "  # å°†è¿”å›å€¼èµ‹ç»™resultå±æ€§\n",
        "  thread.result = (old_response, old_history)\n",
        "  return old_response, old_history\n",
        "\n",
        "# åˆ›å»ºä¸€ä¸ªå­çº¿ç¨‹ï¼Œå¹¶å°†ask_and_ansä½œä¸ºç›®æ ‡å‡½æ•°\n",
        "stop_thread = threading.Thread(target=ask_and_ans, args=(tokenizer, ask, history, max_length, top_p, temperature))\n",
        "# å¯åŠ¨å­çº¿ç¨‹\n",
        "stop_thread.start()\n",
        "# ç­‰å¾…å­çº¿ç¨‹ç»“æŸ\n",
        "stop_thread.join()\n",
        "# è·å–å­çº¿ç¨‹çš„è¿”å›å€¼\n",
        "response, history = stop_thread.result\n",
        "count = count +1\n",
        "\n",
        "#è®¾ç½®æ¸…ç©ºæ ‡å¿—\n",
        "if count > max_turns:\n",
        "  clear_history_flag = True\n"
      ],
      "metadata": {
        "id": "CNygeoOO_dfA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "8298dee0c2e94655bb22fbca2583a3a9",
            "48dc4a84269f41e69a4a5e9767d685c4",
            "2b60a6eae56f4baa8ee1d5efd73a9d9b",
            "d09cab66c8af4bfaa3857d6fa836555f",
            "4c3db9fc83b0443e98fe8206ac4f9dd1",
            "06fffb59b0054caf99e6c9dca75921f7"
          ]
        },
        "outputId": "17b1a5d9-f565-4423-9e8d-d838437c0b95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç¬¬ 3 æ¬¡å¯¹è¯ï¼Œåˆ°è¾¾ 20 æ¬¡åï¼Œä¸‹ä¸€æ¬¡å¯¹è¯æ—¶å°†åˆ é™¤å†å²å¯¹è¯è®°å½•\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='ç»ˆæ­¢å›ç­”', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8298dee0c2e94655bb22fbca2583a3a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='ChatGLM:', layout=Layout(flex='0 1 auto', height='auto', min_height='100px', wâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d09cab66c8af4bfaa3857d6fa836555f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç»ˆæ­¢å›ç­”.\n",
            "ç»ˆæ­¢å›ç­”.\n"
          ]
        }
      ]
    }
  ]
}