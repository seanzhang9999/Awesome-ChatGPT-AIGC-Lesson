# -*- coding: utf-8 -*-
"""ChatGLM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/110kV0l9zeVhfvzqz4WJBF7Okzke2m3c6
"""

# 查看显卡信息
!nvidia-smi

# clone工程路径
!git clone https://github.com/THUDM/ChatGLM-6B.git

# 安装命令行
!pip install kora
from kora import console
console.start()

cd ChatGLM-6B

ls

mkdir modelINT4

cd modelINT4

cd ..

cd modelINT4

# 下载模型文件，INT4量化版本
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/LICENSE
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/MODEL_LICENSE
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/README.md
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/config.json
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/configuration_chatglm.py
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/ice_text.model
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/modeling_chatglm.py
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/pytorch_model.bin
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/quantization.py
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/quantization_kernels.c
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/quantization_kernels_parallel.c
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/tokenization_chatglm.py
!wget https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/tokenizer_config.json

ls

cd ..

# Commented out IPython magic to ensure Python compatibility.
# # 下载miniconda
# %%bash
# MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh
# MINICONDA_PREFIX=/usr/local
# wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT
# chmod +x $MINICONDA_INSTALLER_SCRIPT
# ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX

!which conda # 返回/usr/local/bin/conda

!conda create -n lm python=3.7

!source activate lm
!conda activate lm

!conda info --env

!pip install PyHamcrest==1.9.0
!pip install protobuf==3.19.5

!conda install transformers

!pip install icetk

!pip install cpm_kernels

!pip install gradio

cd ChatGLM-6B

!pip install torch

# 需要安装的其他包
!pip install --upgrade protobuf icetk cpm_kernels

# 首次加载模型以及测试

from transformers import AutoTokenizer, AutoModel
import os
import time
# 指定缓存位置
os.environ["TRANSFORMERS_CACHE"] = "/content/models_cache" 

# 加载模型
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4",trust_remote_code=True).half().cuda()
model = model.eval()

# 推理
print("start conversation")
t = time.time()
response, history = model.chat(tokenizer, "你是谁呢呀？", history=[])
print(response)

print(f'coast:{time.time() - t:.4f}s')

# 循环测试反应速度
import locale
locale.getpreferredencoding = lambda: "UTF-8"

print("start conversation")
for i in range(10):
  t = time.time()
  response, history = model.chat(tokenizer, "你都能做什么呢", history=[])
  # !nvidia-smi
  print(os.system("!nvidia-smi"))
  print(response)
  print(f'coast:{time.time() - t:.4f}s')
  !nvidia-smi

import locale
locale.getpreferredencoding = lambda: "UTF-8"
!pip uninstall wandb
!pip install wandb

"""# New Section"""

